{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "StepExp = namedtuple('StepExperience', ['obs', 'obs_n', 'act', 'logp', 'val', 'rew', 'done'])\n",
    "Experience = namedtuple('Experience', ['obs', 'r2g', 'adv', 'logp', 'act'])\n",
    "\n",
    "class ExpBuffer:\n",
    "    def __init__(self, env, capacity):\n",
    "        obs_dim = env.observation_space.shape\n",
    "        act_dim = int(env.action_space.n)\n",
    "        self.start = 0\n",
    "        self.idx = 0\n",
    "        self.size = 0\n",
    "        self.capacity = capacity\n",
    "\n",
    "        self.obs = torch.zeros((self.capacity, *obs_dim), requires_grad=False).float()\n",
    "        self.obs_n = torch.zeros((self.capacity, *obs_dim), requires_grad=False).float()\n",
    "        self.act = torch.zeros((self.capacity, act_dim), requires_grad=False).long()\n",
    "        self.logp = torch.zeros((self.capacity, 1), requires_grad=False).float()\n",
    "        self.val = torch.zeros((self.capacity, 1), requires_grad=False).float()\n",
    "        self.rew = torch.zeros((self.capacity, 1), requires_grad=False).float()\n",
    "        self.done = torch.zeros((self.capacity, 1), requires_grad=False).float()\n",
    "        self.r2g = torch.zeros((self.capacity, 1), requires_grad=False).float()\n",
    "        self.adv = torch.zeros((self.capacity, 1), requires_grad=False).float()\n",
    "\n",
    "    def store(self, exp):\n",
    "        self.obs[self.idx] = exp.obs\n",
    "        self.obs_n[self.idx] = exp.obs_n\n",
    "        self.act[self.idx] = exp.act\n",
    "        self.logp[self.idx] = exp.logp\n",
    "        self.val[self.idx] = exp.val\n",
    "        self.done[self.idx] = exp.done\n",
    "        self.rew[self.idx] = exp.rew\n",
    "\n",
    "        self.size = min(self.size+1, self.capacity)\n",
    "        self.idx = (self.idx+1) % self.capacity\n",
    "\n",
    "    def store_finished(self, r2g, adv):\n",
    "        self.r2g[self.start:self.size] = r2g[:, None]\n",
    "        self.adv[self.start:self.size] = adv[:, None]\n",
    "        self.start = self.idx\n",
    "\n",
    "    def reset(self):\n",
    "        self.idx = 0\n",
    "        self.size = 0\n",
    "        self.start = 0\n",
    "\n",
    "    def sample(self, bs):\n",
    "        assert self.size >= bs\n",
    "        idxs = np.random.choice(self.size, bs, replace=False)\n",
    "        adv = self.adv[idxs]\n",
    "        ''' normalize advantage\n",
    "        std = adv.std()\n",
    "        mean = adv.mean()\n",
    "        adv = (adv - mean) / std\n",
    "        '''\n",
    "        return Experience(\n",
    "            self.obs[idxs],\n",
    "            self.r2g[idxs],\n",
    "            adv,\n",
    "            self.logp[idxs],\n",
    "            self.act[idxs]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(policy, obs):\n",
    "    logits = policy(obs)\n",
    "    dist = Categorical(logits=logits)\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "    if action.shape == ():\n",
    "        return action.item(), log_prob.item()\n",
    "    return action, log_prob\n",
    "\n",
    "def reward_to_go(rewards, gamma=0.99):\n",
    "    n = len(rewards)\n",
    "    r2g = torch.zeros(n, requires_grad=False)\n",
    "    for i in reversed(range(n)):\n",
    "        r2g[i] = rewards[i] + gamma*(0 if i+1 >= n else r2g[i+1])\n",
    "\n",
    "    return r2g\n",
    "\n",
    "def advantage(rewards, vals, gamma=0.99, lam=0.95):\n",
    "   n = len(rewards)\n",
    "   diff = [rewards[i] + gamma*vals[i+1] - vals[i] for i in range(n-1)]\n",
    "   adv = torch.zeros(n-1, requires_grad=False)\n",
    "   for i in reversed(range(n-1)):\n",
    "      adv[i] = diff[i] + gamma*lam*(0 if i+1 >= n-1 else adv[i+1])\n",
    "\n",
    "   return adv\n",
    "   \n",
    "def step(policy, value, obs, env):\n",
    "    act, logp = get_action(policy, obs)\n",
    "    val = value(obs)\n",
    "    obs_n, rew, term, trunc, _ = env.step(act)\n",
    "    obs_n = torch.from_numpy(obs_n)\n",
    "    done = term or trunc\n",
    "\n",
    "    return StepExp(\n",
    "        obs,\n",
    "        obs_n,\n",
    "        act, \n",
    "        logp,\n",
    "        val,\n",
    "        rew,\n",
    "        done,\n",
    "    )\n",
    "\n",
    "def policy_loss(policy, exp, eps):\n",
    "    act, logp = get_action(policy, exp.obs)\n",
    "    ratio = torch.exp(logp[:,None] - exp.logp)\n",
    "    clip_adv = torch.clamp(ratio, 1-eps, 1+eps)*exp.adv\n",
    "    loss = -(torch.min(ratio*exp.adv, clip_adv)).mean()\n",
    "\n",
    "    return loss\n",
    "\n",
    "def value_loss(value, exp):\n",
    "    diff = (value(exp.obs) - exp.r2g)**2\n",
    "    mse = diff.mean()\n",
    "\n",
    "    return mse\n",
    "\n",
    "def fit_policy(policy, exp, opt, epochs):\n",
    "    losses = []\n",
    "    for i in range(epochs):\n",
    "        opt.zero_grad()\n",
    "        loss = policy_loss(policy, exp, eps=0.2)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses\n",
    "\n",
    "def fit_value(value, exp, opt, epochs):\n",
    "    losses = []\n",
    "    for i in range(epochs):\n",
    "        opt.zero_grad()\n",
    "        loss = value_loss(value, exp)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    return losses\n",
    "\n",
    "def average(arr):\n",
    "    return (sum(arr) / len(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -- rew: 23.011850717362393 policy_loss: -8.358571370442709 value_loss: 282.9984598795573\n",
      "1 -- rew: 18.45877056562149 policy_loss: -6.445404084523519 value_loss: 159.71985880533853\n",
      "2 -- rew: 19.32873544265639 policy_loss: -6.508675829569499 value_loss: 97.06512908935547\n",
      "3 -- rew: 20.701354135166515 policy_loss: -5.657056458791097 value_loss: 235.46366373697916\n",
      "4 -- rew: 20.5018705868721 policy_loss: -5.321012051900228 value_loss: 160.00516764322916\n",
      "5 -- rew: 20.827799978595095 policy_loss: -4.540285237630209 value_loss: 93.8729258219401\n",
      "6 -- rew: 21.860456192747076 policy_loss: -2.2959343592325845 value_loss: 83.25439198811848\n",
      "7 -- rew: 21.492119639331758 policy_loss: 0.09864840656518936 value_loss: 46.71092300415039\n",
      "8 -- rew: 21.275663222673643 policy_loss: -1.0858686208724975 value_loss: 64.11043217976888\n",
      "9 -- rew: 21.160258607766064 policy_loss: -2.4761810938517255 value_loss: 83.94046020507812\n",
      "10 -- rew: 19.552003515334356 policy_loss: -0.985777489344279 value_loss: 22.927072270711264\n",
      "11 -- rew: 19.742394071358902 policy_loss: -1.7443802118301392 value_loss: 85.44296366373698\n",
      "12 -- rew: 21.617280297530325 policy_loss: -3.710659646987915 value_loss: 130.42440999348958\n",
      "13 -- rew: 22.567769150157552 policy_loss: -4.390997378031413 value_loss: 86.64258422851563\n",
      "14 -- rew: 21.53534983090705 policy_loss: 2.443481111526489 value_loss: 107.1283665974935\n",
      "15 -- rew: 24.774487116250647 policy_loss: -0.8349688490231831 value_loss: 93.10203297932942\n",
      "16 -- rew: 23.36320081624118 policy_loss: 1.075617210070292 value_loss: 79.29835154215495\n",
      "17 -- rew: 22.340221762657166 policy_loss: 0.4370546579360962 value_loss: 43.8669179280599\n",
      "18 -- rew: 23.061831951141357 policy_loss: -2.8383970578511555 value_loss: 105.17226816813151\n",
      "19 -- rew: 23.224344835443013 policy_loss: 0.07310807754596074 value_loss: 48.26038564046224\n",
      "20 -- rew: 23.36104097149589 policy_loss: -1.5148544311523438 value_loss: 60.533154805501304\n",
      "21 -- rew: 22.461764491972374 policy_loss: 0.14639216264088947 value_loss: 83.23411356608072\n",
      "22 -- rew: 21.840756218484106 policy_loss: -0.5394544800122579 value_loss: 33.23405049641927\n",
      "23 -- rew: 21.839059540565977 policy_loss: -1.6719549655914308 value_loss: 100.77652435302734\n",
      "24 -- rew: 22.847672971089683 policy_loss: 1.7424141724904378 value_loss: 31.907236353556314\n",
      "25 -- rew: 22.505494505494507 policy_loss: -2.4128106753031413 value_loss: 63.58916193644206\n",
      "26 -- rew: 21.841557842619874 policy_loss: -0.3170089483261108 value_loss: 66.20057678222656\n",
      "27 -- rew: 21.42188087105751 policy_loss: -0.7844112475713094 value_loss: 137.84575602213542\n",
      "28 -- rew: 20.79187817258883 policy_loss: 0.8108494679133097 value_loss: 53.84301605224609\n",
      "29 -- rew: 20.147906504425347 policy_loss: -0.19215014378229778 value_loss: 82.39494120279947\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Adam\n",
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "obs_dim = env.observation_space.shape[0]\n",
    "act_dim = env.action_space.n\n",
    "hid_dim = 256\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "policy = nn.Sequential(*[\n",
    "    nn.Linear(obs_dim, hid_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hid_dim, hid_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hid_dim, act_dim)\n",
    "])\n",
    "\n",
    "value = nn.Sequential(*[\n",
    "    nn.Linear(obs_dim, hid_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hid_dim, hid_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hid_dim, 1)\n",
    "])\n",
    "\n",
    "num_epochs = 100\n",
    "num_steps = 4096\n",
    "\n",
    "policy_opt = Adam(policy.parameters(), lr=3e-4)\n",
    "policy_epochs = 15\n",
    "\n",
    "value_opt = Adam(value.parameters(), lr=3e-4)\n",
    "value_epochs = 15\n",
    "\n",
    "experience = ExpBuffer(env, num_steps)\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    experience.reset()\n",
    "\n",
    "    obs, _ = env.reset()\n",
    "    obs = torch.from_numpy(obs)\n",
    "\n",
    "    eps_rews = []\n",
    "    rewards = []\n",
    "    vals = []\n",
    "\n",
    "    # collect experience\n",
    "    policy.eval()\n",
    "    value.eval()\n",
    "    with torch.no_grad():\n",
    "        for j in range(num_steps):\n",
    "            step_exp = step(policy, value, obs, env)\n",
    "\n",
    "            experience.store(step_exp)\n",
    "            rewards.append(step_exp.rew)\n",
    "            vals.append(step_exp.val.item())\n",
    "            obs = step_exp.obs_n\n",
    "            if step_exp.done or j == num_steps-1:\n",
    "                r2g = reward_to_go(rewards)\n",
    "                if not step_exp.done:\n",
    "                    pred = step(policy, value, obs, env)\n",
    "                    rewards.append(pred.val.item())\n",
    "                    vals.append(pred.val.item())\n",
    "                else:\n",
    "                    rewards.append(0)\n",
    "                    vals.append(0)\n",
    "                adv = advantage(rewards, vals).detach()\n",
    "                experience.store_finished(r2g, adv)\n",
    "\n",
    "                # reset\n",
    "                eps_rews.append(sum(rewards))\n",
    "                rewards = []\n",
    "                vals = []\n",
    "                obs, _ = env.reset()\n",
    "                obs = torch.from_numpy(obs)\n",
    "\n",
    "\n",
    "    # update policy\n",
    "    policy.train()\n",
    "    value.train()\n",
    "    sampled_exp = experience.sample(32)\n",
    "    p_losses = fit_policy(policy, sampled_exp, policy_opt, policy_epochs)\n",
    "    v_losses = fit_value(value, sampled_exp, value_opt, value_epochs)\n",
    "\n",
    "    print(i, '--', f'rew: {sum(eps_rews) / len(eps_rews)}', f'policy_loss: {sum(p_losses) / len(p_losses)}', f'value_loss: {sum(v_losses) / len(v_losses)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.616\n"
     ]
    }
   ],
   "source": [
    "rews = []\n",
    "\n",
    "for i in range(1000):\n",
    "    obs, _ = env.reset()\n",
    "    obs = torch.from_numpy(obs)\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        exp = step(policy, value, obs, env)\n",
    "        rewards.append(exp.rew)\n",
    "        done = exp.done\n",
    "        obs = exp.obs_n\n",
    "    \n",
    "    rews.append(sum(rewards))\n",
    "\n",
    "print(sum(rews) / len(rews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.908\n"
     ]
    }
   ],
   "source": [
    "rand_policy = nn.Sequential(*[\n",
    "    nn.Linear(obs_dim, hid_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hid_dim, act_dim)\n",
    "])\n",
    "\n",
    "rand_value = nn.Sequential(*[\n",
    "    nn.Linear(obs_dim, hid_dim),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(hid_dim, 1)\n",
    "])\n",
    "\n",
    "rews = []\n",
    "\n",
    "for i in range(1000):\n",
    "    obs, _ = env.reset()\n",
    "    obs = torch.from_numpy(obs)\n",
    "    done = False\n",
    "    rewards = []\n",
    "    while not done:\n",
    "        exp = step(rand_policy, rand_value, obs, env)\n",
    "        rewards.append(exp.rew)\n",
    "        done = exp.done\n",
    "        obs = exp.obs_n\n",
    "    \n",
    "    rews.append(sum(rewards))\n",
    "\n",
    "print(sum(rews) / len(rews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1])\n",
      "r1 torch.Size([32, 32])\n",
      "c1 torch.Size([32, 32])\n",
      "-0.7566856145858765\n",
      "r2 torch.Size([32, 1])\n",
      "c2 torch.Size([32, 1])\n",
      "-0.6808624863624573\n"
     ]
    }
   ],
   "source": [
    "ret = experience.sample(32)\n",
    "\n",
    "policy(ret.obs).shape\n",
    "\n",
    "act, logp = get_action(policy, ret.obs)\n",
    "\n",
    "print(ret.adv.shape)\n",
    "\n",
    "ratio_1 = torch.exp(logp[:] - ret.logp)\n",
    "print('r1',ratio_1.shape)\n",
    "clip_adv = torch.clamp(ratio_1, 1-0.2, 1+0.2)*ret.adv\n",
    "print('c1', clip_adv.shape)\n",
    "loss = -(torch.min(ratio_1*ret.adv, clip_adv)).mean()\n",
    "print(loss.item())\n",
    "\n",
    "ratio_1 = torch.exp(logp[:,None] - ret.logp)\n",
    "print('r2', ratio_1.shape)\n",
    "clip_adv = torch.clamp(ratio_1, 1-0.2, 1+0.2)*ret.adv\n",
    "print('c2', clip_adv.shape)\n",
    "loss = -(torch.min(ratio_1*ret.adv, clip_adv)).mean()\n",
    "print(loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value(ret.obs).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeprl",
   "language": "python",
   "name": "deeprl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
